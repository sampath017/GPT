{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append((Path.cwd().parent/\"src\").resolve().as_posix())\n",
    "\n",
    "import finetune.settings as s\n",
    "from finetune.utils import ModelSummary\n",
    "from finetune.models import GPT\n",
    "from finetune.utils import ModelCheckpointManager\n",
    "from finetune.dataloader import UltraChat200kDataLoaderLite\n",
    "from finetune.LoRA import LoRALinear\n",
    "from finetune.utils import instruct_generate\n",
    "from finetune.chat_assistant import ChatAssistant\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 shards for split val\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = UltraChat200kDataLoaderLite(split=\"val\")\n",
    "x, y = dataloader.next_batch()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train tokens: 205,829,073\n",
      "Total val tokens:   50,000,000\n"
     ]
    }
   ],
   "source": [
    "def count_tokens_in_dir(data_dir, split=\"train\"):\n",
    "    \"\"\"\n",
    "    Count total tokens across all .npy shards for a given split (train/val).\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    files = sorted(data_dir.glob(f\"{split}_*.npy\"))\n",
    "    \n",
    "    total_tokens = 0\n",
    "    for f in files:\n",
    "        arr = np.load(f, mmap_mode=\"r\")  # don't load into RAM\n",
    "        total_tokens += arr.shape[0]     # length = number of tokens in that shard\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "# point to your dataset folder\n",
    "data_path = s.ultrachat_200k_data_path\n",
    "\n",
    "train_tokens = count_tokens_in_dir(data_path, \"train\")\n",
    "val_tokens   = count_tokens_in_dir(data_path, \"val\")\n",
    "\n",
    "print(f\"Total train tokens: {train_tokens:,}\")\n",
    "print(f\"Total val tokens:   {val_tokens:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762.939453125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.config[\"dataset\"][\"total_batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15258.7890625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e9 / s.config[\"dataset\"][\"total_batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model_checkpoint_train_step_17000_val_loss_3.08:v0, 1425.29MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:2.6 (540.3MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: /workspace/GPT/models/pretrained_models/model_checkpoint_train_step_17000_val_loss_3.08.pt\n",
      "Model size: 475.03 MB\n",
      "Trainable parameters: 124.53M\n",
      "Non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = GPT()\n",
    "\n",
    "wandb_path='sampath017/GPT3_124M/model_checkpoint_train_step_17000_val_loss_3.08:v0'\n",
    "cache_dir=s.models_root_path/\"pretrained_models\"\n",
    "pretrained_model = ModelCheckpointManager.get_model_from_wandb(pretrained_model, wandb_path, cache_dir, model_type=\"pretrained\")\n",
    "ModelSummary.summary(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 884736 / 125374080 (0.71%)\n"
     ]
    }
   ],
   "source": [
    "# apply LoRA\n",
    "finetuned_model = LoRALinear.apply_lora(pretrained_model, r=16, alpha=32, dropout=0.05,\n",
    "                              target_modules=(\"attn\", \"proj\"))\n",
    "\n",
    "# wandb_path='sampath017/GPT3_124M_instruct/model_checkpoint_train_step_5_val_loss_3.29:v0'\n",
    "# cache_dir=s.models_root_path/\"finetuned_models\"\n",
    "# finetuned_model = ModelCheckpointManager.get_model_from_wandb(finetuned_model, wandb_path, cache_dir, model_type=\"finetuned\")\n",
    "\n",
    "if s.ddp_master_process:\n",
    "    # check trainable params\n",
    "    trainable = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in finetuned_model.parameters())\n",
    "    print(\n",
    "        f\"Trainable params: {trainable} / {total} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where can I find fun?\n",
      "Tay-Shopper: The word “Hey, where can I find it?”\n",
      "-Tay-Shopper: The name of the game\n",
      "The word “Hey, is this?”\n",
      "-Heilayake: That’s a funny sound and a little bit of laughter\n",
      "This is the main character in the story.\n",
      "-Heilayake: Oh, I guess, that’s a little bit of a joke.\n",
      "-Rhyme: Hey, that’s a funny sound and a little bit of a joke: It’s a funny sound\n",
      "This is the main character in the story!\n",
      "-Rhyme: It’s a funny sound and a little bit of an idiot!\n",
      "In this story, in this story, in this story, in this one, in this one, in this one, in this one, we get it is, that’s a funny sound!\n",
      "1) In this story, I’m talking about a little bit of an idiot and what I find it funny!\n",
      "-Rhyme: It’s a\n"
     ]
    }
   ],
   "source": [
    "s = instruct_generate(finetuned_model)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finetuned_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m chat = ChatAssistant(\u001b[43mfinetuned_model\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(chat.ask(\u001b[33m\"\u001b[39m\u001b[33mHello, who are you?\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'finetuned_model' is not defined"
     ]
    }
   ],
   "source": [
    "chat = ChatAssistant(finetuned_model)\n",
    "\n",
    "print(chat.ask(\"Hello, who are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.ask(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.ask(\"Explain why it is funny\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from torchinfo import summary\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from dataset import ShakespearDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from quickai.trainer import Trainer\n",
    "from quickai.logger import WandbLogger\n",
    "\n",
    "from models import BigramLanguageModel\n",
    "from module import BigramLanguageModule\n",
    "from torch.nn import functional as F\n",
    "import settings as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data\")\n",
    "logs_path = Path(\"../logs\")\n",
    "logs_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = WandbLogger(\n",
    "    project_name=s.project_name,\n",
    "    config={\n",
    "        \"model\": s.model,\n",
    "        \"dataset\": s.dataset,\n",
    "        \"max_epochs\": s.max_epochs,\n",
    "        \"optimizer\": s.optimizer,\n",
    "        \"lr_scheduler\": s.lr_scheduler,\n",
    "        \"test_run\": s.test_run,\n",
    "        \"transfer_learning\": s.transfer_learning\n",
    "    },\n",
    "    logs_path=logs_path,\n",
    "    offline=s.wandb_offline\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "# cpu_count = 7\n",
    "\n",
    "dataset = ShakespearDataset(data_path/\"shakespear.txt\", s.dataset[\"context_size\"])\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [s.dataset[\"train_split\"], s.dataset[\"val_split\"]]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=s.dataset[\"batch_size\"], shuffle=True, num_workers=cpu_count)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=s.dataset[\"batch_size\"],  num_workers=cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(\n",
    "    context_size=s.dataset[\"context_size\"],\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    num_embds=s.model[\"num_embds\"],\n",
    "    head_size=s.model[\"head_size\"]\n",
    ")\n",
    "\n",
    "from quickai.utils import model_size\n",
    "\n",
    "model_size(model)\n",
    "# summary(\n",
    "#     model,\n",
    "#     input_size=(s.dataset[\"batch_size\"],\n",
    "#                 *train_dataset[0][0].shape),\n",
    "#     device=\"cpu\",\n",
    "#     mode=\"train\",\n",
    "#     depth=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_scheduler is None!\n"
     ]
    }
   ],
   "source": [
    "module = BigramLanguageModule(dataset.vocab_size, num_embds=64)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    params=module.model.parameters(),\n",
    "    weight_decay=s.optimizer[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    if s.lr_scheduler[\"name\"] == \"OneCycleLR\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            max_lr=s.lr_scheduler[\"max_lr\"],\n",
    "            epochs=s.max_epochs,\n",
    "            steps_per_epoch=len(train_dataloader),\n",
    "        )\n",
    "\n",
    "        print(s.lr_scheduler[\"name\"])\n",
    "except TypeError:\n",
    "    lr_scheduler = None\n",
    "    print(\"lr_scheduler is None!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu!\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    module=module,\n",
    "    logger=logger,\n",
    "    optimizer=optimizer,\n",
    "    callbacks=[],\n",
    "    logs_path=logs_path,\n",
    "    fast_dev_run=s.fast_dev_run,\n",
    "    limit_batches=s.limit_batches,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    save_checkpoint_type=\"best_val\",\n",
    "    num_workers=cpu_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msampath017\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../logs/wandb/run-20250207_104052-uuaumi83</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sampath017/GPT/runs/uuaumi83' target=\"_blank\">wandering-glade-23</a></strong> to <a href='https://wandb.ai/sampath017/GPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sampath017/GPT' target=\"_blank\">https://wandb.ai/sampath017/GPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sampath017/GPT/runs/uuaumi83' target=\"_blank\">https://wandb.ai/sampath017/GPT/runs/uuaumi83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▁▁▁▁▁</td></tr><tr><td>step_train_accuracy</td><td>▄▁▃▁█▇</td></tr><tr><td>step_train_loss</td><td>███▆▁▄</td></tr><tr><td>training_step</td><td>▁▁▂▂▄▄▅▅▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.001</td></tr><tr><td>model_architecture</td><td>BigramLanguageModel(...</td></tr><tr><td>step_train_accuracy</td><td>9.96094</td></tr><tr><td>step_train_loss</td><td>4.03746</td></tr><tr><td>training_step</td><td>6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-glade-23</strong> at: <a href='https://wandb.ai/sampath017/GPT/runs/uuaumi83' target=\"_blank\">https://wandb.ai/sampath017/GPT/runs/uuaumi83</a><br/> View project at: <a href='https://wandb.ai/sampath017/GPT' target=\"_blank\">https://wandb.ai/sampath017/GPT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../logs/wandb/run-20250207_104052-uuaumi83/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'save_best_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gpt-07zi0Xob-py3.12/lib/python3.12/site-packages/quickai/trainer.py:166\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    165\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 166\u001b[0m epoch_train_loss, epoch_train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m measure_time_bool:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gpt-07zi0Xob-py3.12/lib/python3.12/site-packages/quickai/trainer.py:245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 245\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m step_train_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/workspaces/GPT/notebooks/../src/module.py:40\u001b[0m, in \u001b[0;36mBigramLanguageModule.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 40\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, acc\n",
      "File \u001b[0;32m/workspaces/GPT/notebooks/../src/module.py:34\u001b[0m, in \u001b[0;36mBigramLanguageModule.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     32\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39mT)\n\u001b[0;32m---> 34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy(logits, y)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gpt-07zi0Xob-py3.12/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 64 is out of bounds.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun stopped!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/gpt-07zi0Xob-py3.12/lib/python3.12/site-packages/quickai/trainer.py:231\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_dev_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_best_model\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    232\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mlog_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_path, aliases\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'save_best_model'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.fit(train_dataloader, val_dataloader)\n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Run stopped!\")\n",
    "finally:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quickai.utils import load_from_checkpoint\n",
    "\n",
    "# module = BigramLanguageModule(dataset.vocab_size)\n",
    "# module.model, _, _ = load_from_checkpoint(\"../logs/wandb/latest-run/checkpoints/best_val_acc_32.42.pt\", module.model)\n",
    "# module.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HRQCIRSAUNYGKNGVCAG HERSAEEDNIARNKGNDGK:AIUeV\n",
      "NNG:WyCCH\n",
      ":,\n",
      "EOU Ph:aNtG\n",
      "vhehAeihsunt z;\n",
      "c\n",
      "pzLnWhTeOtKoyR :ekreC\n",
      "nKgv.CtKnhhaKI!a\n",
      "rKNCDIW:ipmhyvenpaeiyz,\n",
      "ze,\n",
      "Lzeps\n",
      "I:ernhzs\n",
      ":\n",
      "mNeC\n",
      "EmpbeQ:,\n",
      "rYe\n",
      "U.ytBhR.nBU:Ketch\n",
      "Ihnahtee:y. xh\n",
      ":Kt\n",
      "'hchNe, ohtC U UhSyn\n",
      "Ka\n",
      "o\n",
      "IvIetohmayfh?b. \n",
      "aJeIhUvKmeeCeIsAmPy! N?y3 UA GnR:KpIDNKVeCaIy: KtV':h\n",
      ",\n",
      "n\n",
      "eCnhMenE YKrYyO I:,\n",
      "NR?\n",
      "KCGCGVIRIKNIKSCVCAELNKDNNGEVIAINNGCNIEAIG RNKC:KKGG\n",
      "EILNYD:UEGU\n",
      "NKNTGAGIYN:UCMKC\n",
      "CKEISCKENNGAENCGHRNDDWOCGUIWNLQUNUUUEECSY::\n",
      "KTC\n",
      "IhHEAO:;\n",
      "NNG\n",
      "GGAGM\n"
     ]
    }
   ],
   "source": [
    "def generate(idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -s.dataset[\"context_size\"]:]\n",
    "        logits = module.model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :] \n",
    "        probs = F.softmax(logits, dim=-1) \n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1) \n",
    "\n",
    "    return idx\n",
    "\n",
    "text = dataset._decode(generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500).tolist()[0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8]) torch.Size([64, 8])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 65])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BigramLanguageModel\n",
    "\n",
    "model = BigramLanguageModel(dataset.vocab_size)\n",
    "embds = model(x)\n",
    "embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(8, 8))\n",
    "wei = torch.zeros((8, 8))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow = wei.matmul(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-07zi0Xob-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

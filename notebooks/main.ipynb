{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch import optim\n",
    "\n",
    "import os\n",
    "from dataset import ShakespearDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from quickai.trainer import Trainer\n",
    "from quickai.utils import model_size, load_from_checkpoint\n",
    "from quickai.callbacks import OverfitCallback, EarlyStoppingCallback\n",
    "from quickai.logger import WandbLogger\n",
    "from quickai.dataset import MapDataset\n",
    "\n",
    "from module import BigramLanguageModule\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import settings as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data\")\n",
    "logs_path = Path(\"../logs\")\n",
    "logs_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = WandbLogger(\n",
    "    project_name=s.project_name,\n",
    "    config={\n",
    "        \"model\": s.model,\n",
    "        \"dataset\": s.dataset,\n",
    "        \"max_epochs\": s.max_epochs,\n",
    "        \"optimizer\": s.optimizer,\n",
    "        \"lr_scheduler\": s.lr_scheduler,\n",
    "        \"test_run\": s.test_run,\n",
    "        \"transfer_learning\": s.transfer_learning\n",
    "    },\n",
    "    logs_path=logs_path,\n",
    "    offline=s.wandb_offline\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = os.cpu_count()\n",
    "# cpu_count = 7\n",
    "\n",
    "dataset = ShakespearDataset(data_path/\"shakespear.txt\", s.dataset[\"context_size\"])\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [s.dataset[\"train_split\"], s.dataset[\"val_split\"]]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=s.dataset[\"batch_size\"], shuffle=True, num_workers=cpu_count)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=s.dataset[\"batch_size\"],  num_workers=cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 8]), torch.Size([64, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    x, y = batch\n",
    "    break\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_scheduler is None!\n"
     ]
    }
   ],
   "source": [
    "module = BigramLanguageModule(dataset.num_chars)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    params=module.model.parameters(),\n",
    "    weight_decay=s.optimizer[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    if s.lr_scheduler[\"name\"] == \"OneCycleLR\":\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            max_lr=s.lr_scheduler[\"max_lr\"],\n",
    "            epochs=s.max_epochs,\n",
    "            steps_per_epoch=len(train_dataloader),\n",
    "        )\n",
    "\n",
    "        print(s.lr_scheduler[\"name\"])\n",
    "except TypeError:\n",
    "    lr_scheduler = None\n",
    "    print(\"lr_scheduler is None!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu!\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    module=module,\n",
    "    logger=logger,\n",
    "    optimizer=optimizer,\n",
    "    callbacks=[],\n",
    "    logs_path=logs_path,\n",
    "    fast_dev_run=s.fast_dev_run,\n",
    "    limit_batches=s.limit_batches,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    save_checkpoint_type=\"best_val\",\n",
    "    num_workers=cpu_count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msampath017\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../logs/wandb/run-20250205_110416-qc93nuf0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sampath017/GPT/runs/qc93nuf0' target=\"_blank\">frosty-field-2</a></strong> to <a href='https://wandb.ai/sampath017/GPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sampath017/GPT' target=\"_blank\">https://wandb.ai/sampath017/GPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sampath017/GPT/runs/qc93nuf0' target=\"_blank\">https://wandb.ai/sampath017/GPT/runs/qc93nuf0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per epoch: 35.23 seconds\n",
      "Epoch: 0, train_accuracy: 23.96, val_accuracy: 27.14, lr: 0.0010\n",
      "Epoch: 1, train_accuracy: 27.13, val_accuracy: 27.14, lr: 0.0010\n",
      "Epoch: 2, train_accuracy: 27.12, val_accuracy: 27.15, lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36eab4ec43ea4eec89b276d0ef7ffbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▅▅██</td></tr><tr><td>epoch_train_accuracy</td><td>▁██</td></tr><tr><td>epoch_train_loss</td><td>█▁▁</td></tr><tr><td>epoch_val_accuracy</td><td>▁▄█</td></tr><tr><td>epoch_val_loss</td><td>█▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_train_accuracy</td><td>▁▄▅▆▇▆▇█▆▇▇▇▆▆▇▇▇▇▆▇▇▇▇▇▇▇▆▆▇▆▇▇▇███▇▇█▇</td></tr><tr><td>step_train_loss</td><td>█▆▁▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_val_accuracy</td><td>▄▄▂▂▁▄▁▄▆▂▂▂▄▁▃▅▄▅▃▆▄▃▄▅▅▄▃▅▃▅█▂▄▅▃▅▃▄▂▄</td></tr><tr><td>step_val_loss</td><td>▆▄▅▅▄▅▆▂▆█▅▆▄▃▃▆▄▃▆▆▄▁▃▅▅▄▆▅▃▄▇▇█▄▃▅▅▃▅▅</td></tr><tr><td>training_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>validation_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>epoch_train_accuracy</td><td>27.12474</td></tr><tr><td>epoch_train_loss</td><td>2.45336</td></tr><tr><td>epoch_val_accuracy</td><td>27.1466</td></tr><tr><td>epoch_val_loss</td><td>2.45318</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>model_architecture</td><td>BigramLanguageModel(...</td></tr><tr><td>step_train_accuracy</td><td>27.65152</td></tr><tr><td>step_train_loss</td><td>2.46553</td></tr><tr><td>step_val_accuracy</td><td>25.54348</td></tr><tr><td>step_val_loss</td><td>2.46368</td></tr><tr><td>training_step</td><td>36600</td></tr><tr><td>validation_step</td><td>15687</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-field-2</strong> at: <a href='https://wandb.ai/sampath017/GPT/runs/qc93nuf0' target=\"_blank\">https://wandb.ai/sampath017/GPT/runs/qc93nuf0</a><br/> View project at: <a href='https://wandb.ai/sampath017/GPT' target=\"_blank\">https://wandb.ai/sampath017/GPT</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../logs/wandb/run-20250205_110416-qc93nuf0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    trainer.fit(train_dataloader, val_dataloader)\n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Run stopped!\")\n",
    "finally:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = module.model(idx)\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OMBod myorro's?\n",
      "F br it,\n",
      "SSe litot, bor ELAn IO:\n",
      "Aninatyok myorllly hr thenirat alee bouito, beathoyowoue,\n",
      "\n",
      "\n",
      "We m ttl d f m Bur arsis te?\n",
      "TIO, sir it n'Then, gthe tearuck kigine d.\n",
      "ESnthitherele, a t\n",
      "Te tomange oof mesor ththeds'sel'st t p, indge le'd f k.\n",
      "Pe iourupupoulder squt GO:\n",
      "ongr th t os l u eoror ct, her. Wixt ge t geayssour rgrsin ous thed brdurert, mardalld I hifl:\n",
      "Ant, ses s t'lo,\n",
      "UCOROLListhispporoverosthin\n",
      "Aneweray fay Lea ther mer ur! nars,\n",
      "\n",
      "\n",
      "I atuire herertre a pangat\n",
      "WAnsioin we\n"
     ]
    }
   ],
   "source": [
    "text = dataset._decode(generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500).tolist()[0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-07zi0Xob-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
